(A) Base-DT Model
(A) Hyperparameters:
	criterion: gini
	max_depth: None
	min_samples_split: 2
	random_state: None

(B) Confusion Matrix:
[[48  0  0]
 [ 1 17  0]
 [ 0  0 34]]

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.98      1.00      0.99        48
   Chinstrap       1.00      0.94      0.97        18
      Gentoo       1.00      1.00      1.00        34

    accuracy                           0.99       100
   macro avg       0.99      0.98      0.99       100
weighted avg       0.99      0.99      0.99       100


(D) Accuracy: 0.99
(D) Macro-average F1: 0.987039764359352
(D) Weighted-average F1: 0.9899086892488954

**************************************************

Base-DT Model
Average Accuracy: 0.98
Accuracy Variance: 0.0

Average Macro-average F1: 0.9745370370370371
Macro-average F1 Variance: 0.0

Average Weighted-average F1: 0.98
Weighted-average F1 Variance: 0.0


**************************************************

(A) Top-DT Model
(A) Hyperparameters:
	criterion: entropy
	max_depth: 10
	min_samples_split: 6
	random_state: 0

(B) Confusion Matrix:
[[47  1  0]
 [ 1 17  0]
 [ 0  0 34]]

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.98      0.98      0.98        48
   Chinstrap       0.94      0.94      0.94        18
      Gentoo       1.00      1.00      1.00        34

    accuracy                           0.98       100
   macro avg       0.97      0.97      0.97       100
weighted avg       0.98      0.98      0.98       100


(D) Accuracy: 0.98
(D) Macro-average F1: 0.9745370370370371
(D) Weighted-average F1: 0.98

**************************************************

Top-DT Model
Average Accuracy: 0.98
Accuracy Variance: 0.0

Average Macro-average F1: 0.9745370370370371
Macro-average F1 Variance: 0.0

Average Weighted-average F1: 0.98
Weighted-average F1 Variance: 0.0


**************************************************

(A) Base-MLP Model
(A) Hyperparameters:
	activation: logistic
	hidden_layer_sizes: (100, 100)
	learning_rate: constant
	max_iter: 200
	solver: sgd
	random_state: 0

(B) Confusion Matrix:
[[48  0  0]
 [18  0  0]
 [34  0  0]]

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.48      1.00      0.65        48
   Chinstrap       0.00      0.00      0.00        18
      Gentoo       0.00      0.00      0.00        34

    accuracy                           0.48       100
   macro avg       0.16      0.33      0.22       100
weighted avg       0.23      0.48      0.31       100


(D) Accuracy: 0.48
(D) Macro-average F1: 0.21621621621621623
(D) Weighted-average F1: 0.3113513513513514

**************************************************

Base-MLP Model
Average Accuracy: 0.48
Accuracy Variance: 0.0

Average Macro-average F1: 0.21621621621621623
Macro-average F1 Variance: 0.0

Average Weighted-average F1: 0.3113513513513514
Weighted-average F1 Variance: 0.0


**************************************************

(A) Top-MLP Model
(A) Hyperparameters:
	activation: relu
	hidden_layer_sizes: (10, 10, 10)
	learning_rate: constant
	max_iter: 2000
	solver: adam
	random_state: 0

(B) Confusion Matrix:
[[23 16  9]
 [12  6  0]
 [ 0  4 30]]

(C) Classification Report:
              precision    recall  f1-score   support

      Adelie       0.66      0.48      0.55        48
   Chinstrap       0.23      0.33      0.27        18
      Gentoo       0.77      0.88      0.82        34

    accuracy                           0.59       100
   macro avg       0.55      0.56      0.55       100
weighted avg       0.62      0.59      0.59       100


(D) Accuracy: 0.59
(D) Macro-average F1: 0.5496206494721102
(D) Weighted-average F1: 0.5945670602709718

**************************************************

Top-MLP Model
Average Accuracy: 0.59
Accuracy Variance: 0.0

Average Macro-average F1: 0.5496206494721102
Macro-average F1 Variance: 0.0

Average Weighted-average F1: 0.5945670602709718
Weighted-average F1 Variance: 0.0


**************************************************

